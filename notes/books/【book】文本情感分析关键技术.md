朱俭 著  北京：中国社会科学出版社 2015.11
## 第一章  绪论

### 一 研究背景和研究意义

  Sentiment Analysis：对说话人的观点、态度和情感倾向性进行分析，即分析文本中表达出的主观信息。包括opinion classification，genre classification，sentiment polarity，sentiment classification，semantic orientation，opinion mining，opinion extractive，sentiment analysis等。<br>
  应用：①推荐系统：分析用户在线反馈，挑选出值得推荐的产品和服务，推荐给其他用户。<br>
       ②过滤系统：自动过滤对政府、商业机构不利的文字信息，鉴别撰稿者的情感倾向、政治倾向及态度、观点和看法。<br>
       ③问答系统：判断用户真正的询问意图，识别其中的感情色彩，用合适的语气回复。<br>
### 二 文本情感分析研究现状

  （1）语料阶段<br>
    跨领域情感分析引入STRUCTURAL CORRESPONDENCE LEARNING算法，SCL的目的是：将训练集上的特征尽量对应到测试集中。<br>
  （2）文本预处理阶段<br>
   主观句识别——提取文本的真正表达情感的句子。<br>
   什么是分类器？<br>
   ——一种计算机程序。通过自动学习后，可自动将数据分到已知类别。应用：搜索引擎、检索程序、数据分析、预测。一种机器学习程序，其实质为数学模型，目前有多种分支：Bayes分类器、BP神经网络分类器、决策树算法、SVM算法等。<br>
   （3）特征标注与特征选择阶段<br>
     1）情感特征的标注方法<br>
        情感倾向词典——通过查字典获得词语的褒贬性。<br>
        无监督机器学习方法 —— 基于点互信息计算 文本中抽取关键词和情感基准词 的相似度来对文本的 情感倾向性 进行判别。<br>
     2) 情感特征的选择方法<br>
       句法特征情感分析： N元语法——为有效利用普通领域训练数据，提出一种基于 N元语法分布 的语言模型自适应方法。该方法定义一个小的领域内的高质量种子集 和 一个大的普通领域的质量不稳定的 训练集，将训练集的N元语法分布自适应到 和种子集的N元语法分布想死，以更好的进行特定领域单词识别。  <br>
   (4)情感分类阶段<br>
     1）情感建模的方法<br>
       基于监督学习算法的情感分析仍是主流。包括：朴素贝叶斯、K最邻近、最大熵和支持向量机等。<br>
         K最邻近分类算法：如果一个样本与 样本空间中的 k个最相似（即特征空间中最邻近）的样本中的大多数 属于某一个类别，则该样本也属于这个类别。该方法在定类决策上 只依据最邻近的一个或者几个样本的类别 来决定待分样本所属的类别。对于类域交叉或者重叠较多的待分类样本集来说，K最邻近算法具有一定优势。<br>
       基于规则和无监督的建模方法：局域句法结构和依存关系<br>
   2）情感分析的其他研究点<br>
     除情感分类外，还有评论对象识别，情感倾向性论述的持有者识别，抽取句子中评价词语与目标对象之间的关联关系，评价倾向极性的强度<br>


## 第二章  情感语义块特征

### 第一节  情感特征的定义
#### 一 特征项的选择与权重
   SVM算法简单来讲，将一个文本表示成一个向量就是空间模型，用“特征项-文档”矩阵表示通过多个文本所组成的集合。包括 文档、特征项和权重。<br>
  （1）特征项的选择和权重<br>
    选择何种语言单位作为 文本情感分析的特征项的原则是：<br>
    ①文本在特征项空间中的分布有明显的统计规律；<br>
    ②含有语义信息较多，表达能力强的语言单位；<br>
    ③选择过程容易实现，时间和空间开销小。<br>
     中文以字为语言单位，词由一个或多个字组成，词间没有分隔符。因而，分词是中文信息处理的基础。<br>
     中文分词包含3个过程：词典词语切分、切分排除歧义、未登录词识别。**常见分词方法有：**最大匹配法、最短路径法、全切分法、最大概率法。微软亚洲研究院采用**一体化噪声信道模型**：将字典内词切分、命名实体、实体词和新词识别在一个框架内进行对于不同的分词规范，提出基于TBL自适应分词技术。**问题**：分词规范难以构造出高度一致性的词表。CRF（条件随机场）使用较多，其用分隔和标注数据序列的概率模型构架，可准确反映上下文环境对分隔的影响。<br>
**确定特征权重**<br>
①在文档中出现次数越多，与文档的主题越相关。
②在搜索到的所有文档中，总共出现的次数越多，则其区别文档类别的能力就越弱。<br>
**几个概念：**<br>
 >词语频率fi^j^：特征项i在文档中出现的频率。<br>
  词语倒排文档频率difi^：该词语在文档中分布情况对其重要性的影响。<br>
  归一化银子：对各个分量进行标准化。
  **几种特征项计算方法：**<br>
* TF-IDF(term frequency-inverse document frequency)
  字词的重要性 与在文件中出现的次数成正比，与在语料中出现的频率成反比。<br>
  主要思想：若某次或短语在一篇文章中出现的频率TF高，且在其他文章中出现的频率低，则认为此词语或短语 有很好的类别区分能力。<br>
  IDF的主要思想：如果包含词条t的文档越少，也就是n越小，IDF越大，则说明词条t具有很好的类别区分能力。但这里有一个不足之处：若某一类文档C包含词条t的文档总数为m，其他类文档中包含t的文档数为k，则n=m+k，当m越大，则n越大，IDF的值会越小，则该词条t的类别区分能力不好。**实际上，应该赋予此词条t较高的权重，并作为该类文档的特征词以区分。**应用于搜寻引擎，作为文件与用户查询之间相关度程度的度量或评级。
* 熵权值

#### 二 语义块特征无监督提取
##### 1）基于块的语义分析
**HNC**(hierarchical network of concepts，概念层次网络)语义块定义：
   语义块是下一级语义的构成单位，包含核心部分和说明部分。语义块以其要素命名。认为汉语以“字义基元化，词义组合化”方式构造新词，因此可构建概念表述体系。
HNC就是 以削减模糊为实现目标的第一步 来描述语言感知的过程。认为无限的自然语言语句可以用有限的句类物理表达式表达。
##### 2）无监督方法
###### ①无监督学习
ML（machine learning），根据所获取的经验数据指导后续行为，其中获取经验数据的主体是机器，知道后续行为的也是机器。<br>
*取得经验数据→分析推理，从数据中获得特定特征的概率分布，从分布中抽取泛化的信息→指导后续行为*

>**A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P,if its performance at tasks in T,as measured by P,improves with experiencce E.**<br>
E:经验，已存在的数据，机器处理的对象<br>
T：任务，根据它机器选择要处理的对象及处理的方式方法<br>
P：表现，机器根据它判断学习的有效性。分为 ACCURACY 和 COMPUTATIONAL COMPLEXITY<br>

**无监督学习、监督学习、半监督学习**

###### ②文本理解
文本，特指经济器编码的字符序列。自然语言理解包含  1）语法分析；2）语义分析；3）逻辑推理 三个步骤。
##### 3）无监督分词
实质上，将无标注语料视为训练集进行学习，后将该语料视为测试集进行分割。
>**MI（mutual information，互信息）**指两个事件集合之间的相关性。一般而言，信道中总存在噪声和干扰，信源发出消息x，通过信道后信宿收到由干扰作用引起的某种变形的y。信宿收到y后**推测**信源发出x的概率，这个过程被称为**后验概率p（x|y）**。信源发出x的概率p（x）为**先验概率**。

#### 三 情感语义块特征的生成
“语义块”源自对语料中文本的完全切分。方法特点包括：<br>
>采用完全切分获得候选词汇，自动生成词典；<br>
不揣选切分结果，忠实纪录统计结果，生成语义块库；<br>
根据记录的特点，采用后缀树 Suffix tree 结构，构成词典；<br>
根据句子或提供的上下文 对切分结果 按策略选出最佳拆分结果，做广义块序列。
##### 后缀树生成
*后缀树就是 将一个给定字符串的所有后缀全部压入一个Trie后，将只有单个叶子的节点压缩，形成一棵树。*<br>
步骤如下：<br>
*得到字符串的所有后缀→对所有非空后缀进行排序→将公有前缀进行合并。*
##### 查找最长公共部分
*找到最深非叶节点，最深非叶节点所经历的字符，串起来就是最长重复子串*

### 第二节 情感特种采集系统
引言
![情感特征采集系统](H:\weihoujing\01 Others)




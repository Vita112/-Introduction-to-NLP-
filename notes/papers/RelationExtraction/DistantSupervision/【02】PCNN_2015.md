## distant supervision for relation extraction via piecewise convolutional neural network 2015
### abstract
+ 2 problems arise when using distant supervision for RE
> 1. heuristic alignment may result in wrong label problem
> 2. the noise generated from the feature extraction process can cause poor performance
+ propose PCNNs with multi-instance learning to address the above 2 problems
> for the 1st, treat distant supervision relation extrction **as a multi-instance learning problem**, taking the uncertainty of instance labels into account.

training set consist of many bags, the label of bags:known;the label of instance in bags:unknown.**design an objective function at the bag level**.

> for the 2nd, adopt convolutional architecture with **piecewise max pooling** to learning relevant features automatically.
### 1 introduction
+ **an assume** in distant supervion
> if 2 entities have a relationship in a known knowledge base, the all sentences that mention these 2 entities will express that relationship in some way.
+ 2 shrotcomings of distant supervision for RE task 
> wrong label problem. e.g. sentences which contain the 2 entities but not express the relationship are selected as a training instance.

> error propagation or accumulation. features are derived by NLP tools, but errors exist in NLP tools.<br>
  sentence length : >= 40 words; accuracy of syntactic parsing decreases with increasing sentence length.
 + solutions in this paper
 
 inspired by **Zeng et al.(2014)**: use a single max pooling operation to determine the most significant features. 
 **shortcomings**：reduce the size of the hidden layers too rapidly and cannot capture the sructure information between 2 entities.
 
 devide the convolution results into 3 segments based on the positions of the 2 given entities; 
 design a piecewise max pooling layer instead of a single max pooling layer.
 
 ### 2　related work
 + supervised approches
 > considerd as a multi-class classification problem but suffer from a lack of labeled training data.
 
 Mints et al.(2009):adopted Freebase to perform distant suprevision, faced with wrong label problem when generating 
 training data.
 
 Riedel et al.(2010):**developed the relaxed distant supervision assumption for multi-instance learning**.
 
 effective **but depends strongly on the quality of the designed features**.
 > feature extraction 
 
feature-based methods: need to select a suitable feature set when converting structured representations into feature vectors.

kernel-based methods:enable the use of a large set of features without extracting them explicitly.
convolutional tree kernel; the subsequence kernel; the dependency tree kernel

Zhang and Zhou(2006):incorporate multi-instance learning into traditional Backpropagation and Radial Basis Function(高斯径向基函数) networks; optimizing them by minimizing a sum-of-squares error function.
> Radial Basis Function:
$$ K(x,z)=exp(-\frac{\left \| \left \| x-z \right \| \right \|^{2}}{2\sigma ^{2}}) $$

classification desicion function is as follow:
$$ f(x)=sign(\sum_{i=1}^{N_{s}}\alpha \_{i}^{\ast }y_{i}exp(-\frac{\left \| \left \| x-z \right \| \right \|^{2}}{2\sigma ^{2}})+b^{\ast }) $$

### 3 Methodology
neural network architecture in this paper:
![pcnn_architecture]()

**procedure includes 4 main parts:Vector Representation; Convolution; Piecewise Max Pooling; Softmax Output**.
#### 3.1 Vector Representation
each word token is transformed into a low-dimensional vector by looking up pre-trained word embeddings.

use position features to specify entity pairs, also transformed into vectors by looking up position embeddings.
+ word embeddings

将一个文本中的每个word映射为一个k-dimensional real-valued vector，被证明可以较好地捕获单词的语义和语法信息。initialized with word embeddings, a neural network can converge to better local minima.

**in this paper, use the Skip-gram model(Mikolov et al. 2013)**
+ position embeddings

a PF is defined as the combination of the relative distances from the current word to e1 and e2, and we get 2 position embedding matrixes PF1 and PF2. By looking up the position matrixes, we can transform the relative distances into real valued vectors. 

**in vector representation part, an instance is transformed into a matrix $ S \in  R^{s\times d} $.**
> s: sentence length

> $ d = d_{w}+d_{p}\ast 2 $,  $ d_{w}=4 $ :the size of word embeddings, $ d_{p}=1 $ :the size of position embeddings.
#### 3.2 convolution




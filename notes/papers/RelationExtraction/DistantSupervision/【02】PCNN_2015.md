## distant supervision for relation extraction via piecewise convolutional neural network 2015
### abstract
+ 2 problems arise when using distant supervision for RE
> 1. heuristic alignment may result in wrong label problem
> 2. the noise generated from the feature extraction process can cause poor performance
+ propose PCNNs with multi-instance learning to address the above 2 problems
> for the 1st, treat distant supervision relation extrction **as a multi-instance learning problem**, taking the uncertainty of instance labels into account.

training set consist of many bags, the label of bags:known;the label of instance in bags:unknown.**design an objective function at the bag level**.

> for the 2nd, adopt convolutional architecture with **piecewise max pooling** to learning relevant features automatically without 
complicated NLP preprocessing.
### 1 introduction
+ **an assume** in distant supervion
> if 2 entities have a relationship in a known knowledge base, the all sentences that mention these 2 entities will express that relationship in some way.
+ 2 shrotcomings of distant supervision for RE task 
> wrong label problem. e.g. sentences which contain the 2 entities but not express the relationship are selected as a training instance.

> error propagation or accumulation. features are derived by NLP tools, but errors exist in NLP tools.<br>
  sentence length : >= 40 words; accuracy of syntactic parsing decreases with increasing sentence length.
 + solutions in this paper
 
 inspired by **Zeng et al.(2014)**: use a single max pooling operation to determine the most significant features. 
 **shortcomings**：reduce the size of the hidden layers too rapidly and cannot capture the sructure information between 2 entities.
 
 propose a model named piecewise convolutional neural networks with multi-instance learning, in which the convolution results is devided into 3 segments based on the positions of the 2 given entities, and a piecewise max pooling layer is designed insteading of a single max pooling layer. The piecewise max pooling procedure returns the maximum value over the entire sentence.
 
 ### 2　related work
 + supervised approches
 > considerd as a multi-class classification problem but suffer from a lack of labeled training data.
 
 Mints et al.(2009):adopted Freebase to perform distant suprevision, faced with wrong label problem when generating 
 training data.
 
 Riedel et al.(2010):**developed the relaxed distant supervision assumption for multi-instance learning**.
 
 effective **but depends strongly on the quality of the designed features**.
 > feature extraction 
 
feature-based methods: need to select a suitable feature set when converting structured representations into feature vectors.

kernel-based methods:enable the use of a large set of features without extracting them explicitly.
convolutional tree kernel; the subsequence kernel; the dependency tree kernel

Zhang and Zhou(2006):incorporate multi-instance learning into traditional Backpropagation and Radial Basis Function(高斯径向基函数) networks; optimizing them by minimizing a sum-of-squares error function.
> Radial Basis Function:
$$K(x,z)=exp(-\frac{\left \| \left \| x-z \right \| \right \|^{2}}{2\sigma ^{2}})$$

classification desicion function is as follow:
$$f(x)=sign(\sum_{i=1}^{N_{s}}\alpha \_{i}^{\ast }y_{i}exp(-\frac{\left \| \left \| x-z \right \| \right \|^{2}}{2\sigma ^{2}})+b^{\ast })$$

### 3 Methodology
we incorporate multi-instance learning into a convolutional neural network to fulfill distance supervised RE which is treated as multi-instance problem.
neural network architecture for ditant supervised relation extraction in this paper:
![pcnn_architecture](https://github.com/Vita112/notes_for_NLP/blob/master/notes/papers/RelationExtraction/DistantSupervision/pictures/pcnn_architecture.jpg)

**procedure includes 4 main parts:Vector Representation; Convolution; Piecewise Max Pooling; Softmax Output**.
#### 3.1 Vector Representation
each word token is transformed into a low-dimensional vector by looking up pre-trained word embeddings.

use position features to specify entity pairs, and it is also transformed into vectors by looking up position embeddings.
+ word embeddings

将一个文本中的每个word映射为一个k-dimensional real-valued vector，被证明可以较好地捕获单词的语义和语法信息。initialized with word embeddings, a neural network can converge to better local minima.

**in this paper, use the Skip-gram model(Mikolov et al. 2013)**
+ position embeddings
![position_embedding](https://github.com/Vita112/notes_for_NLP/blob/master/notes/papers/RelationExtraction/DistantSupervision/pictures/position_embedding.png)
a PF is defined as the combination of the relative distances from the current word to e1 and e2, and we get 2 position embedding matrixes PF1 and PF2. By looking up the position matrixes, we can transform the relative distances into real valued vectors. 

**in vector representation part, an instance is transformed into a matrix $S \in  R^{s\times d}$.**
> s: sentence length

> $d = d_{w}+d_{p}\ast 2$, $d_{w}=4$ :the size of word embeddings, $d_{p}=1$ :the size of position embeddings.
#### 3.2 convolution
卷积是关于权重矩阵w 和 输入向量（序列）q 的操作，w又被称为滤波器。以上图为例，滤波器的长度w为3(w=3),
句子S被认为是一个序列，S={q1,q2,……,qs},卷积操作就是取序列q中的每一个w-gram与w 做点积，来得到另一个序列c：
$$c_{j}=wq_{(j-w+1):j}$$
当使用多个滤波器，即W = {w1,w2,……,wn}时，
$$c_{j}=w_{i}q_{(j-w+1):j}\ 1\leq i< n$$
the convolutional result is a matrix C={c1,c2,……,cn}
#### 3.3 piecewise max pooling
max pooling opetations are often applied for combining the features extracted by convolution layers They naturally address variable sentence lengths, and can capture the most significant features in each feature map.
> single max pooling is insufficient for RE, because it reduces the size of the hidden layers too rapidly
and is too coarse to capture fine-grained features for RE.

in this paper, propose piecewise max pooling procedure, and returns the maximum velue in each segment.
in the above example, the convolution result of each filter $c_i$ is devided into 3 segments{$ c_{i1},c_{i2},c_{i3} $},
the piecewise max pooling prodedure is as follows:
$$p_{ij}=max (c_{ij})\ 1\leq i\leq n,1\leq j\leq 3$$
对每一个卷积滤波器的输出，我们得到一个3维向量
$$p_{i}=\{p_{i1},p_{i2},p_{i3}\}$$
接下来，concatenate all vectors $ p_{i:n} $ and apply a non-linear function(tangent). at last,
the piecewise max pooling procedure output a vector:
$$g= tanh(p_{i:n})$$
#### 3.4 softmax output
分段最大池化得到的向量作为softmax分类器的输入，计算每个关系的置信度，下面公式中的o表示网络的最终输出。
$$\mathbf{o}=\mathbf{W_{1}\mathbf{g}}+b$$
我们在倒数第二层使用丢弃法（dropout）用于正则化：首先，在g上进行masking操作(g o r)，r一个有p的概率是1的伯努利随机变量向量。
于是公式变为
$$\mathbf{o}=\mathbf{W_{1}\mathbf{(g\circ r)}}+b$$
每一个输出可看作是对应关系的置信分数,每个分数有可看作是 使用一个softmax操作的条件概率。

**dropput**是指：在输入通过网络向前正向传播，或误差通过网络反向传播的过程中，随机删除隐藏层的部分单元。它**使得网络
拥有冗余的表示**，相比于未使用dropout前模型过于专注某个特征，dropout在每次迭代过程汇中，随机屏蔽一定比例的神经元，使得
模型把注意力分散到其他特征，使得这些特征也具有较好的表现能力。dropout**相当于模型平均和模型组合**，dropout在训练阶段
每次针对某一部分数据生成一个小模型，然后组合这些模型。**减少神经元间的复杂依赖性**：由于输入到网络的数据是随机选取的，
因此不能保证每次喂给网络的输入都包含某两个隐藏节点，这样减少了神经元之间的依赖性，即阻止了某些特征仅在其他某些特征存在时，
必须才有效的情况。
#### 3.5 multi-instance learning
PCNNs-based RE can be stated as a quintuple 
$$\theta =(\mathbf{E},\mathbf{PF}\_{1},\mathbf{PF}\_{2},\mathbf{W},\mathbf{W}\_{1})^{2}$$
the input to the network is a bag, suppose we have
$$T\ bags =\{M_{1},M_{2},\cdots ,M_{T}\}$$
given an input instance $ m_{i}^{j} $,the output vector $\mathbf{o}$ shows the score associated
with relations.our conditional probability is defined as follow:
$$p(r|m_{i}^{j}:\theta )=\frac{e^{o_{r}}}{\sum_{k=1}^{n_{1}}e^{o_{k}}}$$
objective function using cross-entropy at bags levels are as follows:
$$J(\theta )=\sum_{i=1}^{T}logp(y_{i}|m_{i}^{j}:\theta )$$
**maximize J(θ) through stochastic gradient descent over shuffled mini-batches with Adadelta update rule**.
for gradient descent, refers to [gradient_descent.md](https://github.com/Vita112/notes_for_NLP/blob/master/methods-models/gradient_descent.md)

backpropagation with multi-instance learning modifies a network based on bags. Thus in our method, some 
training instances will inevitabely be incorrectly labeled. When using PCNN to predict, a bag is positively
labeled **if and only if** the output of network on at least one of its instances is assigned a positive label.

### 4 experiments
4 steps: 1. introduce dataset an evaluation metrics used

2. test several variants via cross-validation

3. compare the performance of our method to other methods

4. evalution
#### 4.1 dataset and evaluation metrics
**DATASET**:generated by aligning Freebase relations with the NYT corpus, with sentences from the years 2005-2006 used as
the training corpus and sentences from 2007 used as the testing corpus.
**evaluation**:held-out evaluation只比较抽取的关系实例和Freebase关系数据，给出实验的精度/召回率曲线；

manual evaluation：manually check the newly discovered relation instance that are not in Freebase.

#### 4.2 experimental settinigs
+ 预训练词嵌入

**use word2vec model to train the word embeddings on NYT corpus**. to obtain the embeddings of the entities, 
we concatenate the tokens of a entity using the \## operator when the entity has multiple word takens.
+ parameters settings

**study the effects of 2 parameters**: window size w and the number of feature maps n;

use a grid search to **determine the opitimal parameters** an manuallt specify subsets of the parameter spaces

use Adadelta in the **update procedure**, and it relise on 2 main parameters:$\rho $ and $\epsilon $. 

in **dropput procedure**, randomly set the hidden unit activities to 0 with a probability of 0.5.
#### 4.3 comparison with traditional approaches 
+ held-out evaluation

![held-out_avaluation_for_PCNNs+MIL]()
select 3 traditional method for comparison :distant-upervision-based model, multi-instance learning medol, multi-instance multi-label model. (using manually cragted features)

PCNNs + MIL, which denotes model in this paper, learns feature representation automatically via PCNNs. so it alleviate the error propagation.
+ manual evaluation
![manual_evaluation_for_PCNNs+MIL]()
the held-out evaluation suffers from false negatives(missclassifed examples which are actuallt ture relation instances), which lead to a sharp decline in the held-out evaluation precision-recall curves of PCNNs+ MIL.

TO ADDRESS THE above problem, use the manual evaluation. we make sure there is no overlap between the held-out and manual candidates.
and we choose the top N extracted relation instances to calculate the precision.

TABLE 2 shows that the precision is higher than in the held-out evaluation, which indicates that many of the negatives that we predict
are ture relation facts, and this explain the sharp decline in figure 4.
#### 4.4 effect of piecewise max pooling and multi-instance learning
develop **Piecewise max pooling**; 

**incorporate the multi-instance learning into convolutional neural networks** for distant supervised relation extraction.**solving the wrong label problem**.

![comparison_among_several_cnns_methods]()

above picture shows that :

> PCNNs **produce better results** than CNNs;<br>
> PCNNs **achieve higher precision** than CNNs+MIL<br>
> PCNNs+MIL ＞ PCNNs ＞ CNNs+MIL ＞ CNNs







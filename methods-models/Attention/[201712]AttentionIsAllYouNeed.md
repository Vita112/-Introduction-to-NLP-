## Abstract
propose a new simple network architecture：Transformer，完全依赖于attention mechanism。

第一个完全依赖于self-attention来计算(输入和输出)表示的传导模型transduction model。
## 1 Introduction
RNN，LSTM和GRU等曾被光用于解决 诸如语言模型和机器翻译等 序列建模和传导问题transduction problems。但其有一个缺点：**前后隐藏状态的依赖性导致无法进行并行计算**。
## 2 Background
## 3 model architecture
## 4 self-attention
## 5 training
## 6 results
## 7 conclusion

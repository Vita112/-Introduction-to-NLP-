
## n-gram语法
一种统计语言模型的算法，其基本思想是对文本内容按照字节进行大小为n的窗口滑动操作，形成长度为n的字节片段序列。（窗口滑动属于TCP知识，参见博文：https://blog.csdn.net/u011904605/article/details/53033255） 
+ 一个假设-马尔科夫假设
> 一个词出现的概率仅仅依赖于他前面出现的有限个的一个或多个单词。第n个词的出现只与他前面的n-1个词有关，而与其他词没有关系。整个句子出现的概率为每个词出现每个词受前一个影响出现的概率的乘积。

s为一个有意义的句子，由一串特定顺序排列的词（w1w2……wi）组成，i表示字符串的长度，即s的单词个数，则s在整个语料库中出现的可能性p(s)，依据链式法则可分解为：

    p（s）=p（w1）p（w2|w1）p（w3|w1w2）……p（wi|w1w2……wi-1）
    当n=1时，为一元模型
    当n=2是，为二元模型
    n=3是，为三元模型
    n指的是字节片段的长度，当n越小时，模型只考虑临近词语之间的关系；当n越大时，模型更多考虑更长距离的上下文之间的关联关系。
    每一个字节片段是一个gram，对所有出现的gram出现的频度进行统计，按照事先设定好的阈值进行过滤，形成关键gram列表，即该文本的向量特征空间，文本中的每一种gram就是一个特征向量维度。
    
## 使用最大似然估计，计算每一项的条件概率
+ 归一化

对于概率模型来说，归一化指，用某个总数来除，是最后得到的概率的值处于0和1之间，以保持概率的合法性。
+ 极大似然估计（maximum likelihood estimation）

极大似然估计通过计算字节片段在语料库中出现的频数，预估单词wi出现在第i位置的概率。它认为，在句子s中，单词wi出现在 以n为字节片段的 第i位的概率可以表示为：

    p（wi|w1w2……wi-1）=c（w1w2……wi）/c（w1w2……wi-1）
    c（w1w2……wi）指字节片段`w1w2……wi`在整个语料库中出现的次数。

+ 2个缺陷

随着n的增大，模型需要的参数变多，要想得到更为准确的概率则需要更为庞大的语料库，来进行统计，这导致参数空间过大。并且zipf定律告诉我们：语言中大多数词在语料中的出现是稀疏的；只有少量的词，语料库可以提供他们规律的可靠样本。n-gram存在以下两大缺点：<br>
① 参数空间过大；  ②由训练样本不足，导致分布不可靠。n-gram面临 数据稀疏严重的问题。
## 平滑技术
   极大似然估计给训练样本中未观察到的事件赋以0的概率，即若某个n-gram在语料库中未出现，则该n-gram的概率为0。而现实世界中，语言的组合不计其数，同一个意思使用不同的字节来组合的例子数不胜数，我们的训练样本不可能包含所有可能出现的语言组合，也就是说给予训练样本中没有的某种n-gram以0的概率是不合理的。为了解决这个问题，可以通过将观察到的n-gram的一部分概率值分布出去，分配给那些未出现的n-gram。这种概率重新分配的方式，被称为`平滑`或`打折`技术。
+ add-one平滑（拉普拉斯平滑）    

取二元语法的计数矩阵，在归一化之前，给所有计数加一。由于对样本语料中每一种n-gram的计数都增加了1，这直接导致了n元语法计数发生改变，进而影响单词wi出现的概率。如果训练语料中未出现的n-gram数量太多，平滑后，`很多概率量被转移到 概率为0 的那些项目当中`，未出现的n-gram占用了太多概率空间，进而使得概率的可信度下降。
+ add-delta平滑    
给计数加上`小于1`的值，来平滑数据稀疏问题。效果比add-one要好，但是仍然不理想。
+ good-turning打折法

>其基本思想是：利用频率的类别信息对频率进行平滑。

    假设N为样本数据的大小，nr为样本中正好出现r次的事件的数目，此处事件指n-gram=w1w2……wn。
    于是，样本数据大小N=Σnr×r（r=[1,∞]），
    我们另r*=r+1（r=[0,∞]），有r*=（r+1）n(r+1)/nr
    则在样本中出现r次的事件的概率为：
    Pr=r*/N
    于是，Σnr×Pr = 1- n1/N（r>0）
    因此，有n1/N的剩余的概率量就可以 均分给所有未出现的事件（r=0）。
    
适用于大词汇集产生的符合多项式分布的大量的数据观察        
+ interpolation平滑

一种线性差值平滑技术，基本思想是：将高阶模型和低阶模型做线性组合，利用低阶n-gram模型对高阶n-gram，做线性差值。
+ back-off回退模型-Katz平滑

基本思想是：当某一事件在样本中出现的概率大于K（通常为0或1），运用极大似然估计减值来估计其概率，否则，使用低阶的（n-1）-gram概率来代替n-gram概率。
只是这种替代必须受归一化因子α的作用。其一般形式为：


## n-gram语言模型的应用
+ 计算n-gram距离，评估2个字符串之间的差异
+ 基于一定的语料库，预估或评估一个句子是否合理（分词和词性标注）

    

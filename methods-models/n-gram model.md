## N-gram模型
一种统计语言模型的算法，其基本思想是对文本内容 进行大小为N个字节 的窗口滑动操作，形成长度为N的字节片段序列。
（窗口滑动属于TCP知识，参见博文：https://blog.csdn.net/u011904605/article/details/53033255） 

基于人脑联想的过程，其特点是 某个词的出现依赖于其他若干个词，并且来自其他单词的信息越多，预测越准确。**LM(language model)是一个基于概率的判别模型，其输入是一句话(单词的顺序序列)，输出为这句话的概率，即单词的联合概率**。N-gram模型是一种LM，是N个单词的有序集合，且允许单词重复出现。N表示字节片段的长度。
    
    当N=1时，为一元模型
    当N=2是，为二元模型
    当N=3是，为三元模型
    N越小时，模型只考虑临近词语之间的关系；当N大时，模型更多考虑更长距离的上下文之间的关联关系。也可以认为 历史信息越多，对后面的未知信息约束就越强。

每一个字节片段是一个gram，对所有出现的gram出现的频度进行统计，按照事先设定好的阈值进行过滤，形成关键gram列表，即该文本的向量特征空间，文本中的每一种gram就是一个特征向量维度。

对于一个由n个单词组成的句子S=(w1w2……wn)，假设 每个单词wi都依赖于从w1到wi-1的，则
$$P(S)=p(w_{1}\cdots w_{n})=p(w_{1})p(w_{2}|w_{1})\cdots p(w_{n}|w_{n-1}\cdots w_{1}$$

+ 2个缺陷

随着n的增大，模型需要的参数变多，要想得到更为准确的概率则需要更为庞大的语料库，来进行统计，这导致参数空间过大。并且**zipf定律告诉我们：语言中大多数词在语料中的出现是稀疏的**；只有少量的词，语料库可以提供他们规律的可靠样本。Ngram存在以下两大问题(缺陷)：<br>
① 参数空间过大,概率$p(w_{n}|w_{n-1}\cdots w_{1})$的参数由O(n)个； 

②由训练样本不足，数据稀疏严重的问题，导致分布不可靠。    

## 针对问题①参数空间过大 的解决方法：马尔科夫假设markov assumption
> 一个词出现的概率仅仅依赖于他前面出现的有限个的一个或多个单词。第i词的出现只与他前面的i-1个词有关，而与其他词没有关系。

s为一个有意义的句子，由一串特定顺序排列的词（w1w2……wn）组成，n表示字符串的长度，则s在整个语料库中出现的可能性p(s)，依据链式法则可分解为：
$$P(S)=p(w_{1}\cdots w_{n})=\prod p(w_{i}|w_{i-1}\cdots w_{1})\approx \prod p(w_{i}|w_{i-N+1}\cdots w_{i-1})$$

## 使用最大似然估计，计算每一项的条件概率
+ 归一化

对于概率模型来说，归一化指，用某个总数来除，是最后得到的概率的值处于0和1之间，以保持概率的合法性。
+ 极大似然估计（maximum likelihood estimation）

极大似然估计通过计算字节片段在语料库中出现的频数，预估单词wi出现在第i位置的概率。它认为，在句子s中，单词wi出现在 以N字节片段的 第i位的概率可以表示为：

    p（wi|w_i-N+1……wi-1）=c（w_i-N+1……wi）/c（w_i-N+1……wi-1）
    c（w_i-N+1……wi）指字节片段`w_i-N+1……wi`在整个语料库中出现的次数。

## 针对问题②数据稀疏 的解决办法：discounting or smoothing平滑或打折技术
   极大似然估计给训练样本中未观察到的事件赋以0的概率，即若某个N-gram在语料库中未出现，则该N-gram的概率为0。而现实世界中，语言的组合不计其数，同一个意思使用不同的字节来组合的例子数不胜数，我们的训练样本不可能包含所有可能出现的语言组合，也就是说给予训练样本中没有的某种N-gram以0的概率是不合理的。
   
   为了解决这个问题，可以通过将观察到的N-gram的一部分概率值分布出去，分配给那些未出现的N-gram。即 **调整最大似然估计的概率值，使0概率增值，非0概率下降，使分布趋向更加均匀，这种概率重新分配的方式，被称为`平滑`或`打折`技术**。主要有几下几种：
> 1. additive smoothing加值平滑
> 2. backoff and interpolation
> 3. absolute discounting绝对折扣法

+ add-one平滑（拉普拉斯平滑）    

取二元语法的计数矩阵，在归一化之前，给所有计数加1。由于对样本语料中每一种N-gram的计数都增加了1，这直接导致了N元语法计数发生改变，进而影响单词wi出现的概率。如果训练语料中未出现的N-gram数量太多，平滑后，`很多概率量被转移到 概率为0 的那些项目当中`，未出现的N-gram占用了太多概率空间，进而使得概率的可信度下降。
+ add-δ平滑    

给计数加上`小于1`的值，来平滑数据稀疏问题。效果比add-one要好，但是仍然不理想。

*以上两种都属于加值平滑，其思想是：给每一种情况出现的次数加上一个值的δ，0＜δ≤1*。在bigram中有：
$$P(w_{i}|w_{i-1})=\frac{\delta +C(w_{i-1}w_{i})}{V\times \delta +C(w_{i-1})}$$

**在开始以下方法讲解之前，说一下backoff和interpolation的区别。backoff就是 对出现过的N-gram的概率打折后，将折扣部分均摊在 所有未出现的N-gram上；interpolation则是 将折扣部分的概率值均摊到 所有的N-gram上**。
+ good-turning(backoff)

>由古德(I.J.Good)于1953年引用 图灵的方法 而提出。其基本思想是：利用频率的类别信息来平滑频率。对于在语料库中出现r次的事件，用r\*来估算概率，即把非零概率的N-gram的概率值降低，匀给一些低概率N-gram，以修改最大似然估计与真是概率之间的偏离。

一般来说，在语料库中，出现次数r与该次数r对应的词的数量Nr 成反比，即出现次数为r的词的数量要比次数为r+1的词的数量多。假设N为样本库中事件的个数，样本中正好出现r次的事件有Nr个，未登录词即未出现事件的次数有N0个，此处事件指N-gram。则
$$N=\sum_{r=1}^{\infty }rN_{r}$$
出现r次的N-gram在样本空间中的相对频度为r/N。当r非常小时，计算出现r次的N-gram的概率需要使用一个更小的数r\*来估计，有r\*=（r+1）\*N_(r+1)/Nr
则在样本中出现r次的事件的概率为：
P(r)=r*/N
    
适用于大词汇集产生的符合多项式分布的大量的数据观察 .      
+ Katz back-off

基本思想是：当某一事件在样本中出现的概率大于K（通常为0或1），运用极大似然估计减值来估计其概率，否则，当概率为0时，使用低阶的（n-1）-gram概率来代替n-gram概率。
只是这种替代必须受归一化因子α的作用。其一般形式为(*此公式待考证*)：
$$p(w_{i+1}|w_{i-k+1},\cdots ,w_{i})=\lambda \_{w_{i-k+1},\cdots ,w_{i}}\frac{count(w_{i-k+1},\cdots ,w_{i+1})}{count(w_{i-k+1},\cdots ,w_{i})}+(1-\lambda \_{w_{i-k},\cdots ,w_{i}})p(w_{i+1}|w_{i-k},\cdots ,w_{i})$$
+ jelinek-mercer smoothing(interpolation)

一种线性插值平滑技术，基本思想是：将高阶模型和低阶模型做线性组合，利用低阶N-gram模型对高阶N-gram，做线性插值。
+ absolute discouting

church&gale(1991)提出：直接在分子上减去一个常数，在后面加上一项保证概率求和为1.
$$p(w_{i}|w_{i-1})=\frac{count(w_{i-1}w_{i})-d}{count((w_{i-1})}+\lambda(w_{i-1})p(w_{i})$$


## N-gram语言模型的应用
+ 计算N-gram距离，评估2个字符串之间的差异

+ 基于一定的语料库，预估或评估一个句子是否合理（分词和词性标注）

## 使用困惑度perplexity评价ml的好坏
困惑度根据每个词来估计一句话出现的概率，并用句子长度作normalize，其公式如下：
$$PP(S)=P(w_{1}\cdots w_{N})^{-\frac{1}{N}}\\\\=\sqrt\[N]{\frac{1}{p(w_{1}\cdots w_{N})}}\\\\=\sqrt\[N]{\prod_{i=1}^{N}\frac{1}{p(w_{i}|w_{1}w_{2}\cdots w_{i-1})}}$$
其中，p(wi)是第i个词的概率，第一个词就是p(w1|w0),w0是start，一个占位符。困惑度取值范围为(1,+∞)。这个公式表明**困惑度越小，则$p(w_{1}\cdots w_{N})$就越大，说明词序列越合理，语言模型越好**。
> PPL的另一种表示：
$$PP(S)=2^{-\frac{1}{N}\sum \_{x}p(w_{i})log_{2}p(w_{i})}$$
在unigram时，经常使用这种形式，此时wi不再是个单词，而是第i个bigram 或其他单位量。 
 
 ## 传统语言模型的缺点
 1. 平滑技术 或 back-off，这类方法属人工设计规则，规则复杂，且泛化性能低；
 
 2. 参数空间过大。词表为V的n-gram数目为$|V|^{n}$,n每加大1，需要计算的n-gram至少增倍；
 
 3. 没有考虑单词 及单词区段的语义信息，泛化能力差。
 
 4.**本质上，N-gram模型描述的是 有限状态的正则文法语言**，而自然语言是不确定的，因此N-gram模型表达能力有限，**无法较好的处理长距离依赖语言现象**。

参考文献：[CSDN_blog_自然语言处理NLP中的N-gram模型](https://blog.csdn.net/songbinxu/article/details/80209197)
下一步：NNLM，由Bengio于2003年提出，N-gram的进化版。

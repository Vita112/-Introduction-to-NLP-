## N-gram模型
一种统计语言模型的算法，其基本思想是对文本内容 进行大小为N个字节 的窗口滑动操作，形成长度为N的字节片段序列。
（窗口滑动属于TCP知识，参见博文：https://blog.csdn.net/u011904605/article/details/53033255） 

基于人脑联想的过程，其特点是 某个词的出现依赖于其他若干个词，并且来自其他单词的信息越多，预测越准确。LM(language model)是一个基于概率的判别模型，其输入是一句话(单词的顺序序列)，输出为这句话的概率，即单词的联合概率。N-gram模型是一种LM，是N个单词的有序集合，且允许单词重复出现。N表示字节片段的长度。
    当N=1时，为一元模型
    当N=2是，为二元模型
    当N=3是，为三元模型
    N越小时，模型只考虑临近词语之间的关系；当N大时，模型更多考虑更长距离的上下文之间的关联关系。

每一个字节片段是一个gram，对所有出现的gram出现的频度进行统计，按照事先设定好的阈值进行过滤，形成关键gram列表，即该文本的向量特征空间，文本中的每一种gram就是一个特征向量维度。

对于一个由n个单词组成的句子S=(w1w2……wn)，假设 每个单词wi都依赖于从w1到wi-1的，则
$$P(S)=p(w_{1}\cdots w_{n})=p(w_{1})p(w_{2}|w_{1})\cdots p(w_{n}|w_{n-1}\cdots w_{1}$$$

+ 2个缺陷

随着n的增大，模型需要的参数变多，要想得到更为准确的概率则需要更为庞大的语料库，来进行统计，这导致参数空间过大。并且**zipf定律告诉我们：语言中大多数词在语料中的出现是稀疏的**；只有少量的词，语料库可以提供他们规律的可靠样本。Ngram存在以下两大问题(缺陷)：<br>
① 参数空间过大,概率$p(w_{n}|w_{n-1}\cdots w_{1})$的参数由O(n)个；  ②由训练样本不足，数据稀疏严重的问题，导致分布不可靠。    

## 针对问题①的解决方法：马尔科夫假设
> 一个词出现的概率仅仅依赖于他前面出现的有限个的一个或多个单词。第i词的出现只与他前面的i-1个词有关，而与其他词没有关系。

s为一个有意义的句子，由一串特定顺序排列的词（w1w2……wn）组成，n表示字符串的长度，则s在整个语料库中出现的可能性p(s)，依据链式法则可分解为：
$$P(S)=p(w_{1}\cdots w_{n})=\prod p(w_{i}|w_{i-1}\cdots w_{1})\approx \prod p(w_{i}|w_{i-1}\cdots w_{i-N+1})$$

## 使用最大似然估计，计算每一项的条件概率
+ 归一化

对于概率模型来说，归一化指，用某个总数来除，是最后得到的概率的值处于0和1之间，以保持概率的合法性。
+ 极大似然估计（maximum likelihood estimation）

极大似然估计通过计算字节片段在语料库中出现的频数，预估单词wi出现在第i位置的概率。它认为，在句子s中，单词wi出现在 以N字节片段的 第i位的概率可以表示为：

    p（wi|w1w2……wi-1）=c（w1w2……wi）/c（w1w2……wi-1）
    c（w1w2……wi）指字节片段`w1w2……wi`在整个语料库中出现的次数。

## 平滑技术
   极大似然估计给训练样本中未观察到的事件赋以0的概率，即若某个n-gram在语料库中未出现，则该n-gram的概率为0。而现实世界中，语言的组合不计其数，同一个意思使用不同的字节来组合的例子数不胜数，我们的训练样本不可能包含所有可能出现的语言组合，也就是说给予训练样本中没有的某种n-gram以0的概率是不合理的。为了解决这个问题，可以通过将观察到的n-gram的一部分概率值分布出去，分配给那些未出现的n-gram。这种概率重新分配的方式，被称为`平滑`或`打折`技术。
+ add-one平滑（拉普拉斯平滑）    

取二元语法的计数矩阵，在归一化之前，给所有计数加一。由于对样本语料中每一种n-gram的计数都增加了1，这直接导致了n元语法计数发生改变，进而影响单词wi出现的概率。如果训练语料中未出现的n-gram数量太多，平滑后，`很多概率量被转移到 概率为0 的那些项目当中`，未出现的n-gram占用了太多概率空间，进而使得概率的可信度下降。
+ add-delta平滑    
给计数加上`小于1`的值，来平滑数据稀疏问题。效果比add-one要好，但是仍然不理想。
+ good-turning打折法

>其基本思想是：利用频率的类别信息对频率进行平滑。

    假设N为样本数据的大小，nr为样本中正好出现r次的事件的数目，此处事件指n-gram=w1w2……wn。
    于是，样本数据大小N=Σnr×r（r=[1,∞]），
    我们另r*=r+1（r=[0,∞]），有r*=（r+1）n(r+1)/nr
    则在样本中出现r次的事件的概率为：
    Pr=r*/N
    于是，Σnr×Pr = 1- n1/N（r>0）
    因此，有n1/N的剩余的概率量就可以 均分给所有未出现的事件（r=0）。
    
适用于大词汇集产生的符合多项式分布的大量的数据观察        
+ interpolation平滑

一种线性差值平滑技术，基本思想是：将高阶模型和低阶模型做线性组合，利用低阶n-gram模型对高阶n-gram，做线性差值。
+ back-off回退模型-Katz平滑

基本思想是：当某一事件在样本中出现的概率大于K（通常为0或1），运用极大似然估计减值来估计其概率，否则，使用低阶的（n-1）-gram概率来代替n-gram概率。
只是这种替代必须受归一化因子α的作用。其一般形式为：
$$p(w_{i+1}|w_{i-k},\cdots ,w_{i})=\lambda _{w_{i-k},\cdots ,w_{i+1}}\frac{count(w_{i-k},\cdots ,w_{i+1})}{w_{i-k},\cdots ,w_{i-1}}+(1-\lambda \_{w_{i-k},\cdots ,w_{i}})p(w_{i+1}|w_{i-k+1},\cdots ,w_{i})$$
非神经网络的最佳语言模型如下：
$$p(w_{i+1}|w_{i-k},\cdots ,w_{i})=\frac{count(w_{i-k},\cdots ,w_{i+1})-D(count(w{i-k},\cdots ,w_{i+1}))}{w_{i-k},\cdots ,w_{i-1}}+\gamma (w_{i-k},\cdots ,w_{i})p(w_{i+1}|w_{i-k+1},\cdots ,w_{i})$$

## N-gram语言模型的应用
+ 词性标注

计算n-gram距离，评估2个字符串之间的差异
+ 基于一定的语料库，预估或评估一个句子是否合理（分词和词性标注）

## 使用困惑度perplexity评价ml的好坏
困惑度根据每个词来估计一句话出现的概率，并用句子长度作normalize，其公式如下：
$$PP(S)=P(w_{1}\cdots w_{N})^{-\frac{1}{N}}\\=\sqrt\[N]{\frac{1}{p(w_{1}\cdots w_{N})}}\\=\sqrt\[N]{\prod_{i=1}^{N}\frac{1}{p(w_{i}|w_{1}w_{2}\cdots w_{i-1})}}$$
其中，p(wi)是第i个词的概率，第一个词就是p(w1|w0),w0是start，一个占位符。困惑度取值范围为(1,+∞)。这个公式表明**困惑度越小，则$p(w_{1}\cdots w_{N})}$就越大，说明词序列越合理，语言模型越好**。
> PPL的另一种表示：
$$PP(S)=2^{-\frac{1}{N}\sum \_{x}p(w_{i})log_{2}p(w_{i})}$$
在unigram时，经常使用这种形式，此时wi不再是个单词，而是第i个bigram 或其他单位量。 
 
 ## 传统语言模型的缺点
 1. 平滑技术 或 back-off，这类方法属人工设计规则，规则复杂，且泛化性能低；
 
 2. 参数空间过大。词表为V的n-gram数目为$|V|^{n}$,n每加大1，需要计算的n-gram至少增倍；
 
 3. 没有考虑单词 及单词区段的语义信息，泛化能力差。

